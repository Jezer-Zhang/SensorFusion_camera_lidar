{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a610a8e-7797-483c-8f41-a3670348eeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 \tTraining Loss: 0.505214 \tTest Loss: 0.499756 \t63s\n",
      "Epoch: 2/100 \tTraining Loss: 0.503005 \tTest Loss: 0.500319 \t125s\n",
      "Epoch: 3/100 \tTraining Loss: 0.502568 \tTest Loss: 0.499559 \t188s\n",
      "Epoch: 4/100 \tTraining Loss: 0.502893 \tTest Loss: 0.499749 \t251s\n",
      "Epoch: 5/100 \tTraining Loss: 0.502945 \tTest Loss: 0.499369 \t314s\n",
      "Epoch: 6/100 \tTraining Loss: 0.503053 \tTest Loss: 0.499559 \t376s\n",
      "Epoch: 7/100 \tTraining Loss: 0.502945 \tTest Loss: 0.499369 \t439s\n",
      "Epoch: 8/100 \tTraining Loss: 0.502891 \tTest Loss: 0.499749 \t502s\n",
      "Epoch: 9/100 \tTraining Loss: 0.503053 \tTest Loss: 0.499939 \t565s\n",
      "Epoch: 10/100 \tTraining Loss: 0.502782 \tTest Loss: 0.499559 \t627s\n",
      "Epoch: 11/100 \tTraining Loss: 0.502945 \tTest Loss: 0.499749 \t690s\n",
      "Epoch: 12/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499369 \t753s\n",
      "Epoch: 13/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499179 \t816s\n",
      "Epoch: 14/100 \tTraining Loss: 0.502945 \tTest Loss: 0.500129 \t878s\n",
      "Epoch: 15/100 \tTraining Loss: 0.502945 \tTest Loss: 0.499749 \t941s\n",
      "Epoch: 16/100 \tTraining Loss: 0.503000 \tTest Loss: 0.499939 \t1004s\n",
      "Epoch: 17/100 \tTraining Loss: 0.503053 \tTest Loss: 0.499559 \t1067s\n",
      "Epoch: 18/100 \tTraining Loss: 0.502782 \tTest Loss: 0.499749 \t1130s\n",
      "Epoch: 19/100 \tTraining Loss: 0.502945 \tTest Loss: 0.500129 \t1192s\n",
      "Epoch: 20/100 \tTraining Loss: 0.503216 \tTest Loss: 0.500129 \t1255s\n",
      "Epoch: 21/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499749 \t1318s\n",
      "Epoch: 22/100 \tTraining Loss: 0.503107 \tTest Loss: 0.499369 \t1381s\n",
      "Epoch: 23/100 \tTraining Loss: 0.502945 \tTest Loss: 0.499369 \t1444s\n",
      "Epoch: 24/100 \tTraining Loss: 0.502944 \tTest Loss: 0.500508 \t1507s\n",
      "Epoch: 25/100 \tTraining Loss: 0.502999 \tTest Loss: 0.500129 \t1569s\n",
      "Epoch: 26/100 \tTraining Loss: 0.502727 \tTest Loss: 0.498989 \t1632s\n",
      "Epoch: 27/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499559 \t1695s\n",
      "Epoch: 28/100 \tTraining Loss: 0.502890 \tTest Loss: 0.499749 \t1758s\n",
      "Epoch: 29/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499749 \t1821s\n",
      "Epoch: 30/100 \tTraining Loss: 0.502836 \tTest Loss: 0.500318 \t1883s\n",
      "Epoch: 31/100 \tTraining Loss: 0.502944 \tTest Loss: 0.499559 \t1946s\n",
      "Epoch: 32/100 \tTraining Loss: 0.503053 \tTest Loss: 0.500129 \t2009s\n",
      "Epoch: 33/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499559 \t2072s\n",
      "Epoch: 34/100 \tTraining Loss: 0.503162 \tTest Loss: 0.499939 \t2135s\n",
      "Epoch: 35/100 \tTraining Loss: 0.503107 \tTest Loss: 0.500129 \t2198s\n",
      "Epoch: 36/100 \tTraining Loss: 0.502836 \tTest Loss: 0.500508 \t2260s\n",
      "Epoch: 37/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499369 \t2323s\n",
      "Epoch: 38/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499939 \t2386s\n",
      "Epoch: 39/100 \tTraining Loss: 0.502890 \tTest Loss: 0.499749 \t2449s\n",
      "Epoch: 40/100 \tTraining Loss: 0.502944 \tTest Loss: 0.499939 \t2511s\n",
      "Epoch: 41/100 \tTraining Loss: 0.502890 \tTest Loss: 0.499179 \t2574s\n",
      "Epoch: 42/100 \tTraining Loss: 0.510734 \tTest Loss: 0.806756 \t2637s\n",
      "Epoch: 43/100 \tTraining Loss: 0.690808 \tTest Loss: 0.687602 \t2700s\n",
      "Epoch: 44/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t2763s\n",
      "Epoch: 45/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t2825s\n",
      "Epoch: 46/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t2888s\n",
      "Epoch: 47/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t2951s\n",
      "Epoch: 48/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3014s\n",
      "Epoch: 49/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3076s\n",
      "Epoch: 50/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3139s\n",
      "Epoch: 51/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3202s\n",
      "Epoch: 52/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3264s\n",
      "Epoch: 53/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3327s\n",
      "Epoch: 54/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3389s\n",
      "Epoch: 55/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3451s\n",
      "Epoch: 56/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3514s\n",
      "Epoch: 57/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3577s\n",
      "Epoch: 58/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3640s\n",
      "Epoch: 59/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3702s\n",
      "Epoch: 60/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3765s\n",
      "Epoch: 61/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3827s\n",
      "Epoch: 62/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3890s\n",
      "Epoch: 63/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t3953s\n",
      "Epoch: 64/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t4016s\n",
      "Epoch: 65/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t4079s\n",
      "Epoch: 66/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687602 \t4142s\n",
      "Epoch: 67/100 \tTraining Loss: 0.690771 \tTest Loss: 0.687495 \t4204s\n",
      "Epoch: 68/100 \tTraining Loss: 0.528663 \tTest Loss: 0.620900 \t4267s\n",
      "Epoch: 69/100 \tTraining Loss: 0.502922 \tTest Loss: 0.499939 \t4330s\n",
      "Epoch: 70/100 \tTraining Loss: 0.502999 \tTest Loss: 0.500129 \t4393s\n",
      "Epoch: 71/100 \tTraining Loss: 0.503578 \tTest Loss: 0.499749 \t4455s\n",
      "Epoch: 72/100 \tTraining Loss: 0.503108 \tTest Loss: 0.499369 \t4518s\n",
      "Epoch: 73/100 \tTraining Loss: 0.502999 \tTest Loss: 0.500508 \t4581s\n",
      "Epoch: 74/100 \tTraining Loss: 0.503125 \tTest Loss: 0.499749 \t4643s\n",
      "Epoch: 75/100 \tTraining Loss: 0.502890 \tTest Loss: 0.500318 \t4706s\n",
      "Epoch: 76/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499939 \t4769s\n",
      "Epoch: 77/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499939 \t4832s\n",
      "Epoch: 78/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499939 \t4895s\n",
      "Epoch: 79/100 \tTraining Loss: 0.503162 \tTest Loss: 0.499939 \t4957s\n",
      "Epoch: 80/100 \tTraining Loss: 0.502890 \tTest Loss: 0.499559 \t5020s\n",
      "Epoch: 81/100 \tTraining Loss: 0.502836 \tTest Loss: 0.498989 \t5083s\n",
      "Epoch: 82/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499939 \t5146s\n",
      "Epoch: 83/100 \tTraining Loss: 0.503053 \tTest Loss: 0.499369 \t5208s\n",
      "Epoch: 84/100 \tTraining Loss: 0.502782 \tTest Loss: 0.499939 \t5271s\n",
      "Epoch: 85/100 \tTraining Loss: 0.502890 \tTest Loss: 0.499939 \t5334s\n",
      "Epoch: 86/100 \tTraining Loss: 0.502944 \tTest Loss: 0.499939 \t5396s\n",
      "Epoch: 87/100 \tTraining Loss: 0.503053 \tTest Loss: 0.500508 \t5459s\n",
      "Epoch: 88/100 \tTraining Loss: 0.504544 \tTest Loss: 0.499939 \t5521s\n",
      "Epoch: 89/100 \tTraining Loss: 0.503216 \tTest Loss: 0.500129 \t5584s\n",
      "Epoch: 90/100 \tTraining Loss: 0.502999 \tTest Loss: 0.500318 \t5647s\n",
      "Epoch: 91/100 \tTraining Loss: 0.502944 \tTest Loss: 0.499749 \t5710s\n",
      "Epoch: 92/100 \tTraining Loss: 0.502999 \tTest Loss: 0.499559 \t5773s\n",
      "Epoch: 93/100 \tTraining Loss: 0.503053 \tTest Loss: 0.500129 \t5835s\n",
      "Epoch: 94/100 \tTraining Loss: 0.502782 \tTest Loss: 0.499179 \t5898s\n",
      "Epoch: 95/100 \tTraining Loss: 0.502891 \tTest Loss: 0.500129 \t5961s\n",
      "Epoch: 96/100 \tTraining Loss: 0.502836 \tTest Loss: 0.499559 \t6023s\n",
      "Epoch: 97/100 \tTraining Loss: 0.502890 \tTest Loss: 0.499559 \t6086s\n",
      "Epoch: 98/100 \tTraining Loss: 0.502944 \tTest Loss: 0.499369 \t6148s\n",
      "Epoch: 99/100 \tTraining Loss: 0.503053 \tTest Loss: 0.499179 \t6211s\n",
      "Epoch: 100/100 \tTraining Loss: 0.502944 \tTest Loss: 0.499749 \t6274s\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "# resnet 50 damage evaluation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sensorFusionDataset.data_split import split_dataset\n",
    "from testLoop import test_loop\n",
    "from trainLoop import train_loop\n",
    "from visualization import plot_losses, plot_metrics,plotTraining\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "import h5py as h5\n",
    "from datetime import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transform\n",
    "from PIL import Image\n",
    "\n",
    "damaged_img_path = '../../Dataset/lidar_img_16200/masked_image_65' \n",
    "ori_path = '../../Dataset/lidar_img_16200/original_image_gray'\n",
    "\n",
    "damaged_img_list = sorted(glob.glob(damaged_img_path + '/*'))[0:5000]\n",
    "ori_img_list = sorted(glob.glob(ori_path + '/*'))[0:5000]\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, damaged_img_list, ori_list):\n",
    "        self.image_list = []\n",
    "        for path in damaged_img_list:\n",
    "            self.image_list.append((path, 1))\n",
    "        for path in ori_list:\n",
    "            self.image_list.append((path, 0))\n",
    "        self.transform = transform.Compose([\n",
    "    # transform.Resize((224, 224)),\n",
    "    transform.Grayscale(num_output_channels=3), \n",
    "    transform.ToTensor(),\n",
    "        \n",
    "])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.image_list[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "# 创建数据集实例\n",
    "res_dataset = MyDataset(damaged_img_list, ori_img_list)\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = split_dataset(res_dataset, 0.7, 0.2)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "training_loss = []\n",
    "testing_loss = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 加载预训练的ResNet-50模型\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "# 替换最后一层全连接层\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 1),  # 输出为1维\n",
    "    nn.Sigmoid()  # 添加sigmoid激活函数\n",
    ")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "best_model_wts = copy.deepcopy(resnet.state_dict())\n",
    "min_loss = float('inf')\n",
    "save_path = 'resnet_50_evaluation/'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train the model\n",
    "    resnet.train()\n",
    "    train_loss = 0\n",
    "    for i, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet(image)\n",
    "        loss = criterion(outputs, label.float().view(-1, 1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training loss\n",
    "        train_loss += loss.item() * image.size(0)\n",
    "\n",
    "    # Calculate average training loss\n",
    "    train_epoch_loss = train_loss / len(train_loader.dataset)\n",
    "    training_loss.append(train_epoch_loss)\n",
    "\n",
    "    # Test the model\n",
    "    resnet.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = resnet(image)\n",
    "            loss = criterion(outputs, label.float().view(-1, 1))\n",
    "\n",
    "            # Update test loss\n",
    "            test_loss += loss.item() * image.size(0)\n",
    "\n",
    "        # Calculate average test loss\n",
    "        test_epoch_loss = test_loss / len(test_loader.dataset)\n",
    "        testing_loss.append(test_epoch_loss)\n",
    "\n",
    "        # Check if the current model has the lowest test loss\n",
    "        if test_epoch_loss < min_loss:\n",
    "            min_loss = test_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(resnet.state_dict())\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f} \\t{:.0f}s'.format(epoch, epochs, train_epoch_loss, test_epoch_loss,time.time() - start_time ))\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model_wts, 'resnet_50_evaluation/best_model_{}.pth'.format(epochs))\n",
    "print('Best model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6eb29669-2bd0-4466-bd35-d74f2077383c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5897318124771118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, damaged_img_list, ori_list):\n",
    "        self.image_list = []\n",
    "        for path in damaged_img_list:\n",
    "            self.image_list.append((path, 1))\n",
    "        for path in ori_list:\n",
    "            self.image_list.append((path, 0))\n",
    "        self.transform = transform.Compose([\n",
    "    # transform.Resize((224, 224)),\n",
    "    transform.Grayscale(num_output_channels=3), \n",
    "    transform.ToTensor(),\n",
    "        \n",
    "])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.image_list[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "# Resnet 50\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "metric_value_list = []\n",
    "batch_size = 32\n",
    "\n",
    "img_list =  sorted(glob.glob('RecosntructedImages/2_fusion_linear/0.7-0.3/*'))\n",
    "# img_list =  sorted(glob.glob('RecosntructedImages/2_fusion_concatenate/*'))\n",
    "# img_list =  sorted(glob.glob('RecosntructedImages/2_denoising/*'))\n",
    "\n",
    "ValDataset = MyDataset(img_list, [])\n",
    "new_valloader = DataLoader(dataset=ValDataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "# 加载预训练的ResNet-50模型\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "# 替换最后一层全连接层\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 1),  # 输出为1维\n",
    "    nn.Sigmoid()  # 添加sigmoid激活函数\n",
    ")\n",
    "resnet = resnet.to(device)\n",
    "resnet.load_state_dict(torch.load('resnet_50_evaluation/best_model_100.pth'))\n",
    "resnet.to(device)\n",
    "dataiter = iter(new_valloader)\n",
    "\n",
    "image, label = next(dataiter)\n",
    "image = image.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "output = resnet(image)\n",
    "metric = sum(output)/len(output)\n",
    "print(metric.item())\n",
    "\n",
    "\n",
    "# for i, batch in enumerate(val_loader):\n",
    "#     ori_img, noi_img = batch\n",
    "#     test_model_best.eval()\n",
    "#     output = test_model_best(noi_img)\n",
    "#     count = 0\n",
    "#     for j in range(ori_img.size(0)):\n",
    "#         output_np = output[j][0].detach().cpu().numpy()\n",
    "#         count = count + output_np\n",
    "#     ssim_img = round(count / ori_img.size(0), 3)\n",
    "#     metric_value_list.append(ssim_img)\n",
    "\n",
    "# print(metric_value_list)\n",
    "\n",
    "\n",
    "# print(metric_values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bd645-2453-406b-8e11-5fdcbcf5f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLIPS metrics\n",
    "\n",
    "import lpips\n",
    "\n",
    "from sensorFusionDataset.creatDataset import CreateDatasets\n",
    "from evaluation import calculate_frechet_distance\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sensorFusionDataset.creatDataset import CreateDatasets\n",
    "from sensorFusionNetwork.simpleAutoencoder import SimpleAutoencoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset_path = '../../Dataset/lidar_img_16200/damaged_image_gray_all/DamagedDatasetforComparison'\n",
    "noi_img_list = sorted(glob.glob(dataset_path + '/*'))\n",
    "CAM_FRONT_path = '../../Dataset/lidar_img_16200/original_image_gray'\n",
    "img_list = sorted(glob.glob(CAM_FRONT_path + '/*'))[14400:14656]\n",
    "\n",
    "metric_value_list =[]\n",
    "batch_size = 32\n",
    "\n",
    "ValDataset = CreateDatasets(img_list, noi_img_list)\n",
    "new_valloader = DataLoader(dataset=ValDataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "test_model_best = SimpleAutoencoder()\n",
    "# test_model_last.load_state_dict(torch.load('model_save/damage_{}/last_model_{}.pth'.format(damage_label, experimentIndex)))\n",
    "test_model_best.load_state_dict(\n",
    "    torch.load('Experiment2_sequence_inverse/img2img_model/best_model_300_32_20230502_052654.pth')\n",
    ")\n",
    "\n",
    "loss_fn = lpips.LPIPS(net='alex')\n",
    "\n",
    "\n",
    "for i, batch in enumerate(new_valloader):  # i -->index of batch\n",
    "    ori_img, noi_img = batch\n",
    "    test_model_best.eval()\n",
    "    output = test_model_best(noi_img)\n",
    "    count = 0\n",
    "    for j in range(ori_img.size(0)):\n",
    "        # ori_img_np = ori_img[j][0].detach().numpy()\n",
    "        # output_np = output[j][0].detach().numpy()\n",
    "        lpips_img = loss_fn.forward(ori_img, output)\n",
    "        sum_result = (torch.sum(lpips_img)).item()\n",
    "        print(sum_result)\n",
    "        count = count+sum_result\n",
    "        # count = count.item()\n",
    "    lpips_img = round(count/ori_img.size(0),3)\n",
    "    print(lpips_img)\n",
    "    metric_value_list.append(lpips_img)\n",
    "\n",
    "print(f'scuccess: {metric_value_list}')\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c6194-2cac-40b8-8d26-d4eb45aadd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FID\n",
    "\n",
    "# FID\n",
    "from sensorFusionDataset.creatDataset import CreateDatasets\n",
    "from evaluation import calculate_frechet_distance\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sensorFusionDataset.creatDataset import CreateDatasets\n",
    "from sensorFusionNetwork.simpleAutoencoder import SimpleAutoencoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "dataset_path = '../../Dataset/lidar_img_16200/damaged_image_gray_all/DamagedDatasetforComparison'\n",
    "noi_img_list = sorted(glob.glob(dataset_path + '/*'))\n",
    "CAM_FRONT_path = '../../Dataset/lidar_img_16200/original_image_gray'\n",
    "img_list = sorted(glob.glob(CAM_FRONT_path + '/*'))[14400:14656]\n",
    "\n",
    "metric_value_list =[]\n",
    "batch_size = 32\n",
    "\n",
    "ValDataset = CreateDatasets(img_list, noi_img_list)\n",
    "new_valloader = DataLoader(dataset=ValDataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "test_model_best = SimpleAutoencoder()\n",
    "# test_model_last.load_state_dict(torch.load('model_save/damage_{}/last_model_{}.pth'.format(damage_label, experimentIndex)))\n",
    "test_model_best.load_state_dict(\n",
    "    torch.load('Experiment2_sequence_inverse/img2img_model/best_model_300_32_20230502_052654.pth')\n",
    ")\n",
    "for i, batch in enumerate(new_valloader):  # i -->index of batch\n",
    "    ori_img, noi_img = batch\n",
    "    test_model_best.eval()\n",
    "    output = test_model_best(noi_img)\n",
    "    count = 0\n",
    "    for j in range(ori_img.size(0)):\n",
    "        ori_img_np = ori_img[j][0].detach().numpy()\n",
    "        output_np = output[j][0].detach().numpy()\n",
    "        fid_img = calculate_frechet_distance(ori_img_np, output_np)\n",
    "        count = count+fid_img\n",
    "    fid_img = round(count/ori_img.size(0),3)\n",
    "    \n",
    "    metric_value_list.append(fid_img)\n",
    "\n",
    "print(metric_value_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29777201-48b3-4d55-947d-31796f9be6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000236]\n"
     ]
    }
   ],
   "source": [
    "#  regional mse denoising compare\n",
    "\n",
    "from sensorFusionDataset.creatDataset import CreateDatasets\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sensorFusionDataset.creatDataset import CreateDatasets\n",
    "from sensorFusionNetwork.simpleAutoencoder import SimpleAutoencoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dataset_path = 'regional_mse/masked_img'\n",
    "noi_img_list = sorted(glob.glob(dataset_path + '/*'))\n",
    "CAM_FRONT_path = 'regional_mse/ori_img'\n",
    "img_list = sorted(glob.glob(CAM_FRONT_path + '/*'))\n",
    "\n",
    "metric_value_list =[]\n",
    "batch_size = 32\n",
    "\n",
    "ValDataset = CreateDatasets(img_list, noi_img_list)\n",
    "new_valloader = DataLoader(dataset=ValDataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "test_model_best = SimpleAutoencoder()\n",
    "test_model_best.load_state_dict(\n",
    "    # torch.load('denoising_model/best_model_300_32_20230509_104410.pth')\n",
    "    torch.load('Experiment2_sequence_inverse/img2img_model/best_model_300_32_20230509_175834.pth')\n",
    "\n",
    ")\n",
    "\n",
    "# load the mask function\n",
    "\n",
    "def create_mask(image_shape, block_size, num_blocks):\n",
    "    mask = np.ones(image_shape, dtype=np.float32) \n",
    "    block_size_px = (int(image_shape[0] * block_size), int(image_shape[1] * block_size))\n",
    "    np.random.seed(5)  # 设置随机种子，以确保每次生成的随机方块位置相同\n",
    "    \n",
    "    # 限制方块的位置在下半部分\n",
    "    lower_half = int(image_shape[0] / 3)\n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        top_left = np.random.randint(lower_half, image_shape[0] - block_size_px[0] + 1), np.random.randint(0, image_shape[1] - block_size_px[1] + 1)\n",
    "        bottom_right = top_left[0] + block_size_px[0], top_left[1] + block_size_px[1]\n",
    "        mask[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]] = 0  # 将方块区域的像素值设为灰度值 0.0\n",
    "    return mask\n",
    "\n",
    "block_size = 0.2  # 方块大小为原图大小的百分比\n",
    "num_blocks = 6  # 方块数量\n",
    "\n",
    "\n",
    "for i, batch in enumerate(new_valloader):  # i -->index of batch\n",
    "    ori_img, noi_img = batch\n",
    "    test_model_best.eval()\n",
    "    output = test_model_best(noi_img)\n",
    "    count = 0\n",
    "    for j in range(ori_img.size(0)):\n",
    "        ori_img_np = ori_img[j][0].detach().numpy()\n",
    "        output_np = output[j][0].detach().numpy()\n",
    "        mask = create_mask(ori_img_np.shape[:2], block_size, num_blocks)\n",
    "        local_mse = np.mean(np.square(output_np - ori_img_np) * mask)\n",
    "        count = count+local_mse\n",
    "    local_mse = round(count/ori_img.size(0),6)\n",
    "    metric_value_list.append(local_mse)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(metric_value_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1c4e5-7283-4f7d-9cc6-fe614342f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream task\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# 加载预训练的ResNet-50模型\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "# 替换最后一层全连接层\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, 1)  # 输出为1维\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# 训练回归模型\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss))\n",
    "    print('Training finished.')\n",
    "\n",
    "# 定义数据集和数据加载器\n",
    "# 这里假设您已经准备好了您的数据集并将其转换为PyTorch Dataset格式\n",
    "dataset = MyDataset(...)  # 替换为您的数据集\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 调用训练函数进行模型训练\n",
    "train(resnet, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# 评估模型并输出还原程度的度量值\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    metric_values = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            metric_values.extend(outputs.flatten().tolist())\n",
    "    return metric_values\n",
    "\n",
    "# 定义测试数据集和数据加载器\n",
    "# 这里假设您已经准备好了测试数据集并将其转换为PyTorch Dataset格式\n",
    "test_dataset = MyDataset(...)  # 替换为您的测试数据集\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 调用评估函数获取还原程度的度量值\n",
    "metric_values = evaluate(resnet, test_loader)\n",
    "\n",
    "# metric_values包含了输入经过reconstruction的受损图的还原程度度量值\n",
    "print(metric_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.2",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
